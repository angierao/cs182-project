{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "import StringIO\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Holt</td>\n",
       "      <td>Good evening from Hofstra University in Hempst...</td>\n",
       "      <td>9/26/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Audience</td>\n",
       "      <td>(APPLAUSE)</td>\n",
       "      <td>9/26/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>How are you, Donald?</td>\n",
       "      <td>9/26/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Audience</td>\n",
       "      <td>(APPLAUSE)</td>\n",
       "      <td>9/26/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Holt</td>\n",
       "      <td>Good luck to you.</td>\n",
       "      <td>9/26/16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Line   Speaker                                               Text     Date\n",
       "0     1      Holt  Good evening from Hofstra University in Hempst...  9/26/16\n",
       "1     2  Audience                                         (APPLAUSE)  9/26/16\n",
       "2     3   Clinton                               How are you, Donald?  9/26/16\n",
       "3     4  Audience                                         (APPLAUSE)  9/26/16\n",
       "4     5      Holt                                  Good luck to you.  9/26/16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('debate.csv')\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page = urllib.urlopen('http://www.presidency.ucsb.edu/ws/index.php?pid=119181').read()\n",
    "soup = BeautifulSoup(page, \"lxml\")\n",
    "#speech = soup.find_all('span')[6]\n",
    "speech = soup.find_all('p')\n",
    "for sentence in speech:\n",
    "    sentence = str(sentence)[3:-4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_ids = range(165, 199)\n",
    "for i in range(200,210):\n",
    "    valid_ids.append(i)\n",
    "trump_speeches = []\n",
    "for valid_id in valid_ids:\n",
    "    page = urllib.urlopen('http://www.presidency.ucsb.edu/ws/index.php?pid=119' + str(valid_id)).read()\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    #print len(speech)\n",
    "    #speech = soup.find_all('span')[6]\n",
    "    speech = soup.find_all('p')\n",
    "    for result in speech:\n",
    "        result = str(result)[3:-4]\n",
    "        for i, c in enumerate(result):\n",
    "            if ord(c) < 0 or ord(c) > 127:\n",
    "                result = result[:i] + '.' + result[i+1:]\n",
    "        sentences = tokenizer.tokenize(result)\n",
    "        for sent in sentences:\n",
    "            trump_speeches.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5739\n"
     ]
    }
   ],
   "source": [
    "print len(trump_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeInvalidChars(sentences):\n",
    "    lst = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        segs = sentence.split('<i>')\n",
    "        lst.append(''.join([segs[i] for i in range(len(segs)) if i % 2 == 0]))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_speeches = removeInvalidChars(trump_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_ids = range(148, 165)\n",
    "for i in range(497,503):\n",
    "    valid_ids.append(i)\n",
    "for i in range(688,702):\n",
    "    valid_ids.append(i)\n",
    "clinton_speeches = []\n",
    "for valid_id in valid_ids:\n",
    "    page = urllib.urlopen('http://www.presidency.ucsb.edu/ws/index.php?pid=119' + str(valid_id)).read()\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    #speech = soup.find_all('span')[6]\n",
    "    speech = soup.find_all('p')\n",
    "    for result in speech:\n",
    "        result = str(result)[3:-4]\n",
    "        for i, c in enumerate(result):\n",
    "            if ord(c) < 0 or ord(c) > 127:\n",
    "                result = result[:i] + '.' + result[i+1:]\n",
    "                #print result\n",
    "        sentences = tokenizer.tokenize(result)\n",
    "        for sent in sentences:\n",
    "            clinton_speeches.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clinton_speeches = removeInvalidChars(clinton_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#fp = open(\"test.txt\")\n",
    "#data = fp.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_df = pd.DataFrame({}, columns = df.columns)\n",
    "clinton_df = pd.DataFrame({}, columns = df.columns)\n",
    "trump = []\n",
    "clinton = []\n",
    "\n",
    "end_t = 0\n",
    "end_c = 0\n",
    "for i in range(df.shape[0]):\n",
    "#for i in range(5):\n",
    "    if df['Speaker'].values[i] == 'Trump':\n",
    "        speech = df['Text'].values[i]\n",
    "        sentences = tokenizer.tokenize(speech)\n",
    "        for sentence in sentences:\n",
    "            trump_df.loc[end_t] = [end_t, df['Speaker'].values[i], sentence, df['Date'].values[i]]\n",
    "            trump.append(sentence)\n",
    "            end_t += 1\n",
    "    if df['Speaker'].values[i] == 'Clinton':\n",
    "        speech = df['Text'].values[i]\n",
    "        sentences = tokenizer.tokenize(speech)\n",
    "        for sentence in sentences:\n",
    "            clinton_df.loc[end_c] = [end_c, df['Speaker'].values[i], sentence, df['Date'].values[i]]\n",
    "            clinton.append(sentence)\n",
    "            end_c += 1\n",
    "            \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I want to recognize the Institute for all you do to inspire the next generation of Latino leaders.', 'Last year, I had the chance to spend time with some of the CHCI interns and fellows.', 'It really was like seeing the future of America in one room.', \"I can't wait to see everything that they achieve.\", \"And I can't wait, if I'm fortunate enough to be president, to put some of them to work! \", 'And I want to thank all my friends in the Congressional Hispanic Caucus.', 'You fight every single day to lift up the Latino community ... when the cameras are rolling and when the cameras are off, at home in your districts and here in Washington.', \"And no one understands better than you the pivotal moment we're at right now ... not just for Latinos but for our country.\", 'My friends in the caucus have traveled to every battleground state, registered voters.', \"You've stayed focused no matter what kind of outlandish and offensive comments we have heard from my opponent and his supporters.\", 'By the way, I personally think a taco truck on every corner sounds absolutely delicious.', \"Now, here's a confession: Running for president is never easy, but it shouldn't be, right?\", 'But tonight I have the ultimate challenge: speaking after President Obama.', \"He's always a tough act to follow, in more ways than one.\", \"I, for one, don't think the President gets the credit he deserves for rescuing our economy from another Great Depression.\", \"Think of what we've achieved these last eight years.\", 'American businesses have created 15 million new jobs since the recession.', 'Twenty million Americans have health coverage ... and no one has seen a bigger drop in uninsured rates under the Affordable Care Act than Latino Americans.', 'We got more good news this week.', 'A report came out showing that poverty is going down, and incomes for American families are going up, and Latino families have seen the biggest increase of all.']\n"
     ]
    }
   ],
   "source": [
    "print clinton_speeches[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n"
     ]
    }
   ],
   "source": [
    "print len(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_word = \"NEW\"\n",
    "begin_sentence = \"BEGIN\"\n",
    "end_sentence = \"END\"\n",
    "for i in trump_speeches:\n",
    "    trump.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7759\n"
     ]
    }
   ],
   "source": [
    "print len(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in clinton_speeches:\n",
    "    clinton.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8406\n"
     ]
    }
   ],
   "source": [
    "print len(clinton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16165\n"
     ]
    }
   ],
   "source": [
    "name = 'astro_sentences.txt'\n",
    "f = open(name,'w')\n",
    "'''\n",
    "for sentence in trump:\n",
    "    f.write(sentence+',0\\n')\n",
    "for sentence in clinton:\n",
    "\tf.write(sentence + ',1\\n')\n",
    "'''\n",
    "tot = 0\n",
    "t = 0\n",
    "c = 0\n",
    "cont = True\n",
    "while cont:\n",
    "    if np.random.uniform(low=0.0, high=1.0) < .5:\n",
    "        f.write(trump[t] + ',0\\n')\n",
    "        tot += 1\n",
    "        t += 1\n",
    "    else:\n",
    "        f.write(clinton[c] + ',1\\n')\n",
    "        tot += 1\n",
    "        c += 1\n",
    "    if t == len(trump):\n",
    "        while c < len(clinton):\n",
    "            f.write(clinton[c] + ',1\\n')\n",
    "            tot += 1\n",
    "            c += 1\n",
    "        cont = False\n",
    "    elif c == len(clinton):\n",
    "        while t < len(trump):\n",
    "            f.write(trump[t] + ',0\\n')\n",
    "            tot += 1\n",
    "            t += 1\n",
    "        cont = False\n",
    "\n",
    "f.close()\n",
    "print tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7759\n"
     ]
    }
   ],
   "source": [
    "print len(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trump = [\"%s %s %s\" % (begin_sentence, s, end_sentence) for s in trump]\n",
    "\n",
    "trump_words = [nltk.word_tokenize(sent) for sent in trump]\n",
    "#clinton_words = [nltk.word_tokenize(sent) for sent in clinton]\n",
    "print type(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*trump_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 2200\n",
    "new_word = \"NEW\"\n",
    "begin_sentence = \"BEGIN\"\n",
    "end_sentence = \"END\"\n",
    "\n",
    "vocab = word_freq.most_common(vocab_size-1)\n",
    "\n",
    "index_to_word = [w[0] for w in vocab]\n",
    "index_to_word.append(new_word)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i, sent in enumerate(trump_words):\n",
    "    trump_words[i] = [w if w in word_to_index else new_word for w in sent]\n",
    "    \n",
    "    \n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in trump_words])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in trump_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "     \n",
    "    def __init__(self, word_d, hidden_d=100, bptt_end=4):\n",
    "        self.word_d = word_d\n",
    "        self.bptt_end = bptt_end\n",
    "        self.hidden_d = hidden_d\n",
    "        \n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_d), np.sqrt(1./word_d), (hidden_d, word_d))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_d), np.sqrt(1./hidden_d), (word_d, hidden_d))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_d), np.sqrt(1./hidden_d), (hidden_d, hidden_d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fwd_propagate(self, x):\n",
    "    total_time = len(x)\n",
    "    s = np.zeros((total_time + 1, self.hidden_d))\n",
    "    s[-1] = np.zeros(self.hidden_d)\n",
    "    o = np.zeros((total_time, self.word_d))\n",
    "    for t in range(total_time):\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]\n",
    " \n",
    "RNN.fwd_propagate = fwd_propagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    return np.argmax(self.fwd_propagate(x)[0], axis = 1)\n",
    " \n",
    "RNN.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2200)\n",
      "[[ 0.0004566   0.00045177  0.00045222 ...,  0.0004606   0.00045072\n",
      "   0.00045805]\n",
      " [ 0.00045548  0.00045312  0.00045196 ...,  0.00045784  0.00044956\n",
      "   0.00045599]\n",
      " [ 0.0004525   0.00045689  0.0004547  ...,  0.00044912  0.00045292\n",
      "   0.00045084]\n",
      " ..., \n",
      " [ 0.00045087  0.00045228  0.00045041 ...,  0.00045097  0.00045611\n",
      "   0.00045182]\n",
      " [ 0.00044937  0.00045358  0.00045047 ...,  0.00045237  0.0004638\n",
      "   0.00045149]\n",
      " [ 0.0004564   0.00045696  0.00045649 ...,  0.00046163  0.00045309\n",
      "   0.00045458]]\n"
     ]
    }
   ],
   "source": [
    "model = RNN(vocab_size)\n",
    "o, s = model.fwd_propagate(X_train[10])\n",
    "print o.shape\n",
    "print o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1480 1480 1089 1279  508 1787  380  453 1555 1740 1508 1899 1903   70 1508\n",
      " 1780 1073  537 1901 2139 1460 1145 1192  729 1473 1508 1760  719  848 1296]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_tot(self, x, y):\n",
    "    loss = 0\n",
    "    for i in range(len(y)):\n",
    "        o, s = self.fwd_propagate(x[i])\n",
    "        accurate_pred = o[range(len(y[i])), y[i]]\n",
    "        loss += -1 * np.sum(np.log(accurate_pred))\n",
    "    return loss\n",
    " \n",
    "def loss(self, x, y):\n",
    "    tot = np.sum([len(yi) for yi in y])\n",
    "    return self.loss_tot(x,y) / tot\n",
    " \n",
    "RNN.loss_tot = loss_tot\n",
    "RNN.loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss: 7.696213\n",
      "Empirical loss: 7.695186\n"
     ]
    }
   ],
   "source": [
    "print \"Expected Loss: %f\" % np.log(vocab_size)\n",
    "print \"Empirical loss: %f\" % model.loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bwd_propagate(self, x, y):\n",
    "    total_time = len(y) \n",
    "    o, s = self.fwd_propagate(x)\n",
    "\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    \n",
    "    delta_o = o\n",
    "    delta_o[range(len(y)), y] -= 1.\n",
    "    \n",
    "    for t in reversed(range(total_time)):\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        a = max(0, t-self.bptt_end)\n",
    "        for step in reversed(range(a, t+1)):\n",
    "            dLdW += np.outer(delta_t, s[step-1])              \n",
    "            dLdU[:,x[step]] += delta_t\n",
    "            delta_t = (1 - (s[step-1] ** 2)) * self.W.T.dot(delta_t)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    " \n",
    "RNN.bwd_propagate = bwd_propagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def calc_grad(self, x, y, learning_rate):\n",
    "    # Find gradients\n",
    "    dLdU, dLdV, dLdW = self.bwd_propagate(x, y)\n",
    "    \n",
    "    # Update parameters\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    " \n",
    "RNN.calc_grad = calc_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd_params(model, X_train, y_train, \n",
    "                   learning_rate=0.005, \n",
    "                   nepoch=100, \n",
    "                   evaluate_cycle=5):\n",
    "    losses = []\n",
    "    num_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        if (epoch % evaluate_cycle == 0):\n",
    "            # evaluate\n",
    "            loss = model.loss(X_train, y_train)\n",
    "            losses.append((num_seen, loss))\n",
    "            print \"Loss after seen=%d epoch=%d: %f\" % (num_seen, epoch, loss)\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5 \n",
    "            sys.stdout.flush()\n",
    "\n",
    "        for i in range(len(y_train)):\n",
    "            model.calc_grad(X_train[i], y_train[i], learning_rate)\n",
    "            num_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after seen=0 epoch=0: 7.696420\n"
     ]
    }
   ],
   "source": [
    "model = RNN(vocab_size)\n",
    "losses = sgd_params(model, X_train, y_train, nepoch=1, evaluate_cycle=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_text(model):\n",
    "    sent = [word_to_index[begin_sentence]]\n",
    "    while not sent[-1] == word_to_index[end_sentence]:\n",
    "        prob_word = model.fwd_propagate(sent)[0]\n",
    "        sampled_word = word_to_index[new_word]\n",
    "        \n",
    "        while sampled_word == word_to_index[new_word]:\n",
    "            samples = np.random.multinomial(1, prob_word[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        sent.append(sampled_word)\n",
    "\n",
    "    sent_str = [index_to_word[x] for x in sent[1:-1]]\n",
    "    return sent_str\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN American now , soul close up for countries , and far the families .\n",
      "BEGIN This there going to support is and trillion that will deserves work and America federal do .\n",
      "BEGIN caused are going to over percent November , .\n",
      "BEGIN Just will will that paying country days streets and the good-paying Post in scandal every save American child Hillary and People for tired .\n",
      "BEGIN We will am back taxes moms years being family States of last supporting and our cartels , absolute of the long through into a 30 .\n",
      "BEGIN She , at used my 45 gun saved , and , I will going looked have and lift and over for people to together citizens jobs .\n",
      "BEGIN She many been a buy Again jobs out , Root to going murders employers be 'm .\n",
      "BEGIN We are Make America trade your are trade laughing our trade , us will not to inner America .\n"
     ]
    }
   ],
   "source": [
    "sentences = 8\n",
    "min_len = 7\n",
    " \n",
    "for i in range(sentences):\n",
    "    sent = []\n",
    "\n",
    "    while len(sent) < min_len:\n",
    "        sent = generate_text(model)\n",
    "    print \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
